{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Various Approaches to Prompt Enigneering\n",
    "This notebook will show you how to carry out prompt engineering and solve different types of problems using Large Language Models. All the examples have been run using OpenAI's api service, leveragring their latest GPT3 based models. \n",
    "\n",
    "The approaches we will cover are following:\n",
    "\n",
    "1. Few Shot Prompting\n",
    "2. Chain of Thoughts Prompting - https://arxiv.org/pdf/2201.11903.pdf\n",
    "3. Self Consistency Prompting - https://arxiv.org/pdf/2203.11171.pdf\n",
    "4. ReAct - Reason and Act prompting - https://arxiv.org/pdf/2210.03629.pdf\n",
    "5. PAL - Program aided Language Models - https://arxiv.org/pdf/2211.10435.pdf\n",
    "6. MRKL Systems - Modular Reasoning, Knowledge and Language  - https://arxiv.org/pdf/2205.00445.pdf\n",
    "7. Self Ask with Search - https://arxiv.org/pdf/2210.03350.pdf\n",
    "\n",
    "The libraries we will use are two\n",
    "1. OpenAI apis - https://platform.openai.com/docs/introduction\n",
    "2. LangChain - https://langchain.readthedocs.io/en/latest/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup api keys to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# setup OPENAI Key\n",
    "# if you do not have a key, signup and generate one here - https://platform.openai.com/signup\n",
    "# you may need to use your credit card and then create an api key after that\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"<YOUR-OPENAAI-API-KEY>\"\n",
    "\n",
    "# Setup Search capability to be able to search using Google and various other services\n",
    "# Visit this link and follow instructions to create an api key - https://serpapi.com/\n",
    "# Free tier allows upto 100 searches per month which is more than sufficient for this demo purpose\n",
    "os.environ[\"SERPAPI_API_KEY\"] = \"<YOUR-SERPAPI-API-KEY>\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A simple Prompt Example\n",
    "We use a simple prompt to classify the sentiment of a tweet using a direct call to OpenAi's api via the library provided by OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "prompt = '''\n",
    "Decide whether a Tweet's sentiment is positive, neutral, or negative.\n",
    "\n",
    "Tweet: \"I loved the new Batman movie!\"\n",
    "Sentiment:\n",
    "'''\n",
    "\n",
    "response = openai.Completion.create(\n",
    "  model=\"text-davinci-003\",\n",
    "  prompt=prompt,\n",
    "  temperature=0,\n",
    "  max_tokens=60,\n",
    "  top_p=1.0,\n",
    "  frequency_penalty=0.5,\n",
    "  presence_penalty=0.0\n",
    ")\n",
    "\n",
    "print(response[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chain of Thoughts (CoT)\n",
    "Chain of Thoughts prompt engineering example using OpenAI's python library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A: The cafeteria started with 23 apples. They used 20 to make lunch, leaving 3 apples. They bought 6 more, so they now have 9 apples. The answer is 9.\n"
     ]
    }
   ],
   "source": [
    "prompt = '''\n",
    "Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\n",
    "\n",
    "A: Roget started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5+6=11. The answer is 11.\n",
    "\n",
    "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
    "'''\n",
    "\n",
    "response = openai.Completion.create(\n",
    "  model=\"text-davinci-003\",\n",
    "  prompt=prompt,\n",
    "  temperature=0,\n",
    "  max_tokens=60,\n",
    "  top_p=1.0,\n",
    "  frequency_penalty=0.5,\n",
    "  presence_penalty=0.0\n",
    ")\n",
    "\n",
    "print(response[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Few Shot prompt engineering \n",
    "In this example we show how to use few-shot approach with the help of [LangChain](https://github.com/hwchase17/langchain) library. It is a very comprehensive library which helps design simple to complex LLM based systems with minimal code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Model Prompt ###\n",
      "Give the antonym of every input\n",
      "\n",
      "\n",
      "Word: happy\n",
      "Antonym: sad\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Word: tall\n",
      "Antonym: short\n",
      "\n",
      "\n",
      "\n",
      "Word: big\n",
      "Antonym:\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate, FewShotPromptTemplate\n",
    "\n",
    "# First, create the list of few shot examples.\n",
    "examples = [\n",
    "    {\"word\": \"happy\", \"antonym\": \"sad\"},\n",
    "    {\"word\": \"tall\", \"antonym\": \"short\"},\n",
    "]\n",
    "\n",
    "# Next, we specify the template to format the examples we have provided.\n",
    "# We use the `PromptTemplate` class for this.\n",
    "example_formatter_template = \"\"\"\n",
    "Word: {word}\n",
    "Antonym: {antonym}\\n\n",
    "\"\"\"\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"word\", \"antonym\"],\n",
    "    template=example_formatter_template,\n",
    ")\n",
    "\n",
    "# Finally, we create the `FewShotPromptTemplate` object.\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    # These are the examples we want to insert into the prompt.\n",
    "    examples=examples,\n",
    "    # This is how we want to format the examples when we insert them into the prompt.\n",
    "    example_prompt=example_prompt,\n",
    "    # The prefix is some text that goes before the examples in the prompt.\n",
    "    # Usually, this consists of intructions.\n",
    "    prefix=\"Give the antonym of every input\",\n",
    "    # The suffix is some text that goes after the examples in the prompt.\n",
    "    # Usually, this is where the user input will go\n",
    "    suffix=\"Word: {input}\\nAntonym:\",\n",
    "    # The input variables are the variables that the overall prompt expects.\n",
    "    input_variables=[\"input\"],\n",
    "    # The example_separator is the string we will use to join the prefix, examples, and suffix together with.\n",
    "    example_separator=\"\\n\\n\",\n",
    ")\n",
    "\n",
    "# We can now generate a prompt using the `format` method.\n",
    "print(\"### Model Prompt ###\")\n",
    "print(few_shot_prompt.format(input=\"big\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Model Output ###\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' cloudy'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# instantiate the openai default model - text-davinci-003\n",
    "llm = OpenAI()\n",
    "\n",
    "# set up a chain to be able to run the specific model with specific prompt\n",
    "chain = LLMChain(llm=llm, prompt=few_shot_prompt)\n",
    "\n",
    "# run chain for one input\n",
    "print(\"### Model Output ###\")\n",
    "chain.run(\"sunny\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Synthetic data generation using few-shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Model Output ###\n",
      "\n",
      "\n",
      "Word: generous\n",
      "Antonym: stingy\n",
      "\n",
      "Word: loud\n",
      "Antonym: quiet\n",
      "\n",
      "Word: abundant\n",
      "Antonym: scarce\n"
     ]
    }
   ],
   "source": [
    "examples = [\n",
    "    {\"word\": \"happy\", \"antonym\": \"sad\"},\n",
    "    {\"word\": \"tall\", \"antonym\": \"short\"},\n",
    "    {\"word\": \"sunny\", \"antonym\": \"cloudy\"}\n",
    "]\n",
    "\n",
    "example_formatter_template = \"\"\"\n",
    "Word: {word}\n",
    "Antonym: {antonym}\\n\n",
    "\"\"\"\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"word\", \"antonym\"],\n",
    "    template=example_formatter_template,\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    suffix=\"Add three other examples.\",\n",
    "    input_variables=[],\n",
    ")\n",
    "\n",
    "llm = OpenAI()\n",
    "chain = LLMChain(llm=llm, prompt=few_shot_prompt)\n",
    "print(\"### Model Output ###\")\n",
    "print(chain.predict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chain of Thought Prompting using LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Model Output ###\n",
      " Adding all the odd numbers (15, 5, 13, 7, 1) gives 41. The answer is False.\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\n",
    "A: Adding all the odd numbers (9, 15, 1) gives 25. The answer is False.\n",
    "\n",
    "The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1. \n",
    "A:\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "llm = OpenAI()\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "print(\"### Model Output ###\")\n",
    "print(chain.predict())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Self Consistency Prompting\n",
    "It is an extension of the CoT wherein the decoding (running the model/generating the output from LLM) is done multiple times using the same CoT prompt and then the most consistent/frequent answer is chosen as the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Model Output ###\n",
      "Output 1\n",
      " When you were 6, your sister was 3 (half of 6). Now you are 70, which means your sister is 70 - 3 = 67 years old. The answer is 67.\n",
      "Output 2\n",
      " At age 6, my sister was half my age, so she was 3. Now I am 70, so she is 70 - 3 = 67 years old. The answer is 67.\n",
      "Output 3\n",
      " When I was 6 my sister was half my age. This means she was 6 / 2 = 3 years old. 70 years later she is 3 + 70 = 73 years old. The answer is 73.\n"
     ]
    }
   ],
   "source": [
    "# Table 17 from self-consistency paper - https://arxiv.org/pdf/2203.11171.pdf\n",
    "template = \"\"\"\n",
    "Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\n",
    "A: We start with 15 trees. Later we have 21 trees. The difference must be the number of trees they planted. So, they must have planted 21 - 15 = 6 trees. The answer is 6.\n",
    "\n",
    "Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
    "A: There are 3 cars in the parking lot already. 2 more arrive. Now there are 3 + 2 = 5 cars. The answer is 5.\n",
    "\n",
    "Q: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\n",
    "A: Leah had 32 chocolates and Leah’s sister had 42. That means there were originally 32 + 42 = 74 chocolates. 35 have been eaten. So in total they still have 74 - 35 = 39 chocolates. The answer is 39.\n",
    "\n",
    "Q: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?\n",
    "A: Jason had 20 lollipops. Since he only has 12 now, he must have given the rest to Denny. The number of lollipops he has given to Denny must have been 20 - 12 = 8 lollipops. The answer is 8.\n",
    "\n",
    "Q: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?\n",
    "A: He has 5 toys. He got 2 from mom, so after that he has 5 + 2 = 7 toys. Then he got 2 more from dad, so in total he has 7 + 2 = 9 toys. The answer is 9.\n",
    "\n",
    "Q: There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?\n",
    "A: There are 4 days from monday to thursday. 5 computers were added each day. That means in total 4 * 5 = 20 computers were added. There were 9 computers in the beginning, so now there are 9 + 20 = 29 computers. The answer is 29.\n",
    "\n",
    "Q: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf balls did he have at the end of wednesday?\n",
    "A: Michael initially had 58 balls. He lost 23 on Tuesday, so after that he has 58 - 23 = 35 balls. On Wednesday he lost 2 more so now he has 35 - 2 = 33 balls. The answer is 33.\n",
    "\n",
    "Q: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\n",
    "A: She bought 5 bagels for $3 each. This means she spent 5\n",
    "\n",
    "Q: When I was 6 my sister was half my age. Now I’m 70 how old is my sister?\n",
    "A:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "llm = OpenAI()\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "print(\"### Model Output ###\")\n",
    "for i in range(3):\n",
    "  print(f\"Output {i+1}\\n {chain.predict()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ReAct: Synergizing Reasoning and Acting in Language Models \n",
    "It is an approach which integrates task specific actions which could be doing some calculation, search the web, querying the underlying database. The outcome of such an action generates an observation which then leads to a next cycle of Thought->Action->Observation. The model goes through multiple such cycles finally producing the answer.\n",
    "\n",
    "In the example below we will use the ability to search wikipedia for specific entries and lookup information on the pages returned by the `search`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought 1: I need to search Nicholas Ray and Elia Kazan, find their professions, then\n",
      "find the profession they have in common.\n",
      "Action 1: Search[Nicholas Ray]\u001b[0m\n",
      "Observation 1: \u001b[36;1m\u001b[1;3mNicholas Ray (born Raymond Nicholas Kienzle Jr., August 7, 1911 – June 16, 1979) was an American film director, screenwriter, and actor best known for the 1955 film Rebel Without a Cause. He is appreciated for many narrative features produced between 1947 and 1963 including They Live By Night, In A Lonely Place, Johnny Guitar, and Bigger Than Life, as well as an experimental work produced throughout the 1970s titled We Can't Go Home Again, which was unfinished at the time of Ray's death.\n",
      "Ray's compositions within the CinemaScope frame and use of color are particularly well-regarded and he was an important influence on the French New Wave, with Jean-Luc Godard famously writing in a review of Bitter Victory, \"... there is cinema.  And the cinema is Nicholas Ray.\"\u001b[0m\n",
      "Thought 2:\u001b[32;1m\u001b[1;3m Professions of Nicholas Ray are director, screenwriter, and actor. I need to search Elia Kazan next and find his professions.\n",
      "Action 2: Search[Elia Kazan]\u001b[0m\n",
      "Observation 2: \u001b[36;1m\u001b[1;3mElia Kazan (; born Elias Kazantzoglou (Greek: Ηλίας Καζαντζόγλου); September 7, 1909 – September 28, 2003) was an American film and theatre director, producer, screenwriter and actor, described by The New York Times as \"one of the most honored and influential directors in Broadway and Hollywood history\".Born in Constantinople (now Istanbul), to Cappadocian Greek parents, his family came to the United States in 1913. After attending Williams College and then the Yale School of Drama, he acted professionally for eight years, later joining the Group Theatre in 1932, and co-founded the Actors Studio in 1947. With Robert Lewis and Cheryl Crawford, his actors' studio introduced \"Method Acting\" under the direction of Lee Strasberg. Kazan acted in a few films, including City for Conquest (1940).His films were concerned with personal or social issues of special concern to him. Kazan writes, \"I don't move unless I have some empathy with the basic theme.\" His first such \"issue\" film was Gentleman's Agreement (1947), with Gregory Peck, which dealt with antisemitism in America. It received eight Oscar nominations and three wins, including Kazan's first for Best Director. It was followed by Pinky (1949), one of the first films in mainstream Hollywood to address racial prejudice against African Americans. A Streetcar Named Desire (1951), an adaptation of the stage play which he had also directed, received twelve Oscar nominations, winning four, and was Marlon Brando's breakthrough role. Three years later, he directed Brando again in On the Waterfront, a film about union corruption on the New York harbor waterfront. It also received 12 Oscar nominations, winning eight. In 1955, he directed John Steinbeck's East of Eden, which introduced James Dean to movie audiences.\n",
      "A turning point in Kazan's career came with his testimony as a witness before the House Committee on Un-American Activities in 1952 at the time of the Hollywood blacklist, which brought him strong negative reactions from many friends and colleagues. His testimony helped end the careers of former acting colleagues Morris Carnovsky and Art Smith, along with the work of playwright Clifford Odets. Kazan and Odets had made a pact to name each other in front of the committee. Kazan later justified his act by saying he took \"only the more tolerable of two alternatives that were either way painful and wrong.\" Nearly a half-century later, his anti-Communist testimony continued to cause controversy. When Kazan was awarded an honorary Oscar in 1999, dozens of actors chose not to applaud as 250 demonstrators picketed the event.Kazan influenced the films of the 1950s and 1960s with his provocative, issue-driven subjects. Director Stanley Kubrick called him, \"without question, the best director we have in America, [and] capable of performing miracles with the actors he uses.\": 36  Film author Ian Freer concludes that even \"if his achievements are tainted by political controversy, the debt Hollywood—and actors everywhere—owes him is enormous.\" In 2010, Martin Scorsese co-directed the documentary film A Letter to Elia as a personal tribute to Kazan.\u001b[0m\n",
      "Thought 3:\u001b[32;1m\u001b[1;3m Professions of Elia Kazan are director, producer, screenwriter, and actor. So profession Nicholas Ray and Elia Kazan have in common is director, screenwriter, and actor.\n",
      "Action 3: Finish[director, screenwriter, actor]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'director, screenwriter, actor'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import OpenAI, Wikipedia\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents.react.base import DocstoreExplorer\n",
    "docstore=DocstoreExplorer(Wikipedia())\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Search\",\n",
    "        func=docstore.search,\n",
    "        description=\"Try to search for wiki page.\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Lookup\",\n",
    "        func=docstore.lookup,\n",
    "        description=\"Lookup a term in the page, imitating cmd-F functionality\"\n",
    "    )\n",
    "]\n",
    "\n",
    "llm = OpenAI(temperature=0, model_name=\"text-davinci-003\")\n",
    "react = initialize_agent(tools, llm, agent=\"react-docstore\", verbose=True)\n",
    "\n",
    "question = \"What profession does Nicholas Ray and Elia Kazan have in common?\"\n",
    "react.run(question)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PAL: Program-aided Language Models. \n",
    "While LLMs are good at step-by-step breaking down a math or logic problem via CoT approach, LLMs often make math and logical mistakes even when problem is decomposed correctly. In PAL the intermediate reasoning steps are generated as programs which are offloaded to a solution step which runs these intermediate programs using a runtime such as python interpreter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new PALChain chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mdef solution():\n",
      "    \"\"\"Jan has three times the number of pets as Marcia. Marcia has two  more pets than Cindy. If Cindy has four pets, how many total pets do the three have?\"\"\"\n",
      "    cindy_pets = 4\n",
      "    marcia_pets = cindy_pets + 2\n",
      "    jan_pets = marcia_pets * 3\n",
      "    total_pets = cindy_pets + marcia_pets + jan_pets\n",
      "    result = total_pets\n",
      "    return result\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'28'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import PALChain\n",
    "from langchain import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name='code-davinci-002', temperature=0, max_tokens=512)\n",
    "pal_chain = PALChain.from_math_prompt(llm, verbose=True)\n",
    "\n",
    "question = \"Jan has three times the number of pets as Marcia. Marcia has two  more pets than Cindy. If Cindy has four pets, how many total pets do the three have?\"\n",
    "\n",
    "pal_chain.run(question)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MRKL Systems \n",
    "MRKL stands for Modular Reasoning, Knowledge and Language. It is fairly similar to ReAct in terms of combining LLMs with external tools. In code below the model will reason about a problem to act using one of the following tools a) Google Search via SerpApi ; b) Query a database for some information and c) Calculator implemented via LLM based generation of python code from natural language description of the calculation and running the generated code through a python REPL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find out who Leo DiCaprio's girlfriend is and then calculate her age raised to the 0.43 power.\n",
      "Action: Search\n",
      "Action Input: \"Who is Leo DiCaprio's girlfriend?\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mDiCaprio had a steady girlfriend in Camila Morrone. He had been with the model turned actress for nearly five years, as they were first said to ...\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to find out Camila Morrone's age\n",
      "Action: Search\n",
      "Action Input: \"How old is Camila Morrone?\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m25 years\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to calculate 25 raised to the 0.43 power\n",
      "Action: Calculator\n",
      "Action Input: 25^0.43\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new LLMMathChain chain...\u001b[0m\n",
      "25^0.43\u001b[32;1m\u001b[1;3m\n",
      "```python\n",
      "import math\n",
      "print(math.pow(25, 0.43))\n",
      "```\n",
      "\u001b[0m\n",
      "Answer: \u001b[33;1m\u001b[1;3m3.991298452658078\n",
      "\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 3.991298452658078\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: Camila Morrone is Leo DiCaprio's girlfriend and her current age raised to the 0.43 power is 3.991298452658078.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Camila Morrone is Leo DiCaprio's girlfriend and her current age raised to the 0.43 power is 3.991298452658078.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import LLMMathChain, OpenAI, SerpAPIWrapper, SQLDatabase, SQLDatabaseChain\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "search = SerpAPIWrapper()\n",
    "llm_math_chain = LLMMathChain(llm=llm, verbose=True)\n",
    "db = SQLDatabase.from_uri(\"sqlite:///Chinook.db\")\n",
    "db_chain = SQLDatabaseChain(llm=llm, database=db, verbose=True)\n",
    "tools = [\n",
    "    Tool(\n",
    "        name = \"Search\",\n",
    "        func=search.run,\n",
    "        description=\"useful for when you need to answer questions about current events. You should ask targeted questions\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Calculator\",\n",
    "        func=llm_math_chain.run,\n",
    "        description=\"useful for when you need to answer questions about math\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"FooBar DB\",\n",
    "        func=db_chain.run,\n",
    "        description=\"useful for when you need to answer questions about FooBar. Input should be in the form of a question containing full context\"\n",
    "    )\n",
    "]\n",
    "mrkl = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True)\n",
    "mrkl.run(\"Who is Leo DiCaprio's girlfriend? What is her current age raised to the 0.43 power?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Another example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find out the artist's full name, if they are in the FooBar database, and the total duration of their albums in the database.\n",
      "Action: Search\n",
      "Action Input: \"The Storm Before the Calm\" artist\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mThe Storm Before the Calm (stylized in all lowercase) is the tenth (and eighth international) studio album by Canadian-American singer-songwriter Alanis ...\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now need to check if Alanis Morissette is in the FooBar database.\n",
      "Action: FooBar DB\n",
      "Action Input: Is Alanis Morissette in the FooBar database?\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new SQLDatabaseChain chain...\u001b[0m\n",
      "Is Alanis Morissette in the FooBar database? \n",
      "SQLQuery:\u001b[32;1m\u001b[1;3m SELECT Name FROM Artist WHERE Name LIKE '%Alanis Morissette%';\u001b[0m\n",
      "SQLResult: \u001b[33;1m\u001b[1;3m[('Alanis Morissette',)]\u001b[0m\n",
      "Answer:\u001b[32;1m\u001b[1;3m Yes, Alanis Morissette is in the FooBar database.\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Observation: \u001b[38;5;200m\u001b[1;3m Yes, Alanis Morissette is in the FooBar database.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now need to find out what albums of hers are in the FooBar database and what is the total duration of all such albums.\n",
      "Action: FooBar DB\n",
      "Action Input: What albums of Alanis Morissette are in the FooBar database and what is the total duration of all such albums?\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new SQLDatabaseChain chain...\u001b[0m\n",
      "What albums of Alanis Morissette are in the FooBar database and what is the total duration of all such albums? \n",
      "SQLQuery:\u001b[32;1m\u001b[1;3m SELECT Album.Title, SUM(Track.Milliseconds) AS TotalDuration FROM Album INNER JOIN Track ON Album.AlbumId = Track.AlbumId WHERE Album.ArtistId = (SELECT ArtistId FROM Artist WHERE Name = 'Alanis Morissette') GROUP BY Album.Title;\u001b[0m\n",
      "SQLResult: \u001b[33;1m\u001b[1;3m[('Jagged Little Pill', 3450925)]\u001b[0m\n",
      "Answer:\u001b[32;1m\u001b[1;3m The albums of Alanis Morissette in the FooBar database are 'Jagged Little Pill' and the total duration of all such albums is 3,450,925 milliseconds.\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Observation: \u001b[38;5;200m\u001b[1;3m The albums of Alanis Morissette in the FooBar database are 'Jagged Little Pill' and the total duration of all such albums is 3,450,925 milliseconds.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
      "Final Answer: Alanis Morissette is the artist who recently released an album called 'The Storm Before the Calm' and her albums in the FooBar database are 'Jagged Little Pill' with a total duration of 3,450,925 milliseconds.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Alanis Morissette is the artist who recently released an album called 'The Storm Before the Calm' and her albums in the FooBar database are 'Jagged Little Pill' with a total duration of 3,450,925 milliseconds.\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mrkl.run(\"What is the full name of the artist who recently released an album called 'The Storm Before the Calm' and are they in the FooBar database? If so, what albums of theirs are in the FooBar database and what is the total duration of all such albums?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Self-Ask-With-Search \n",
    "This approach is about composability - getting language models to perform compositional reasoning tasks where the overall solution depends on correctly composing the answers to sub-problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m Yes.\n",
      "Follow up: When was superconductivity discovered?\u001b[0m\n",
      "Intermediate answer: \u001b[36;1m\u001b[1;3m1911\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mFollow up: Who was president of the U.S. in 1911?\u001b[0m\n",
      "Intermediate answer: \u001b[36;1m\u001b[1;3mWilliam Howard Taft was elected the 27th President of the United States (1909-1913) and later became the tenth Chief Justice of the United States (1921-1930), the only person to have served in both of these offices.\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mSo the final answer is: William Howard Taft\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'William Howard Taft'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import OpenAI, SerpAPIWrapper\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "search = SerpAPIWrapper()\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Intermediate Answer\",\n",
    "        func=search.run,\n",
    "        description=\"useful for searching\"\n",
    "    )\n",
    "]\n",
    "\n",
    "self_ask_with_search = initialize_agent(tools, llm, agent=\"self-ask-with-search\", verbose=True)\n",
    "self_ask_with_search.run(\"Who was president of the U.S. when superconductivity was discovered?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9f3118a39b876d9661f0c44bf4b558e984b7919aba49173e18c42c1b2d40d255"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
